import os
import csv
import time
import traceback
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from webdriver_manager.chrome import ChromeDriverManager

# Supprimer les logs TensorFlow Lite
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

URL = "https://juriscassation.cspj.ma/ArretAppels/SearchArretAppel"
OUTPUT_CSV = "resultats_table.csv"
TARGET_PAGE = 120  # D√©finir la page cible


def wait_for_table(driver):
    wait = WebDriverWait(driver, 30)
    table_xpath = '/html/body/div[2]/div[2]/div/section/div/div/div[2]/div/table'
    wait.until(EC.presence_of_element_located((By.XPATH, table_xpath)))
    table = driver.find_element(By.XPATH, table_xpath)
    wait.until(lambda d: len(table.find_elements(By.CSS_SELECTOR, 'tbody tr')) > 0)
    return table


def extract_table_data(driver, table):
    # Extrait en-t√™tes
    headers = [th.text.strip() for th in table.find_elements(By.CSS_SELECTOR, 'thead th')]
    rows = []
    for tr in table.find_elements(By.CSS_SELECTOR, 'tbody tr'):
        try:
            cells = [td.text.strip() for td in tr.find_elements(By.TAG_NAME, 'td')]
            if cells:
                rows.append(cells)
        except StaleElementReferenceException:
            print("‚ö†Ô∏è √âl√©ment de ligne devenu obsol√®te lors de l'extraction des cellules.")
            continue  # Passer √† la ligne suivante
    return headers, rows


def fetch_all_pages(driver, url, target_page):
    driver.get(url)
    wait = WebDriverWait(driver, 30)

    # √âtape 1 : Filtre initial (optionnel)
    try:
        btn_xpath = '/html/body/div[2]/div[1]/section/div/div[2]/div[2]/form/div[4]/div[1]/div/button[1]'
        btn = driver.find_element(By.XPATH, btn_xpath)
        driver.execute_script("arguments[0].click();", btn)
        print("‚úÖ Filtre initial cliqu√©.")
    except Exception:
        print("‚ö†Ô∏è Filtre initial non cliqu√© (optionnel).")

    # √âtape 2 : Clic sur "ÿ®ÿ≠ÿ´"
    try:
        search_btn = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[contains(text(),'ÿ®ÿ≠ÿ´')]")))
        driver.execute_script("arguments[0].click();", search_btn)
        print("‚úÖ Bouton 'ÿ®ÿ≠ÿ´' cliqu√©.")
    except Exception:
        raise RuntimeError("‚ùå Impossible de cliquer sur 'ÿ®ÿ≠ÿ´'.")

    # Scraper page 1
    print("üìÑ Scraping page 1...")
    table = wait_for_table(driver)
    headers, all_rows = extract_table_data(driver, table)

    # Pagination par clic sur la fl√®che "suivant"
    page = 2
    while page <= target_page:
        try:
            # Trouver le bouton '>' de pagination
            next_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'li.PagedList-skipToNext a[rel="next"]')))
            driver.execute_script("arguments[0].click();", next_btn)
            print(f"‚û°Ô∏è Passage √† la page {page}...")
            time.sleep(1)  # pause pour l'AJAX

            # R√©cup√©rer le tableau frais apr√®s la navigation
            table = wait_for_table(driver)
            _, rows = extract_table_data(driver, table)
            all_rows.extend(rows)
            page += 1
        except (NoSuchElementException, TimeoutException):
            print(f"üõë Le bouton 'suivant' n'est plus trouv√© √† la page {page-1} ou le chargement a pris trop de temps.")
            break

    return headers, all_rows


def write_csv(filename, headers, rows):
    with open(filename, mode='w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(headers)
        writer.writerows(rows)


if __name__ == '__main__':
    options = Options()
    # options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.maximize_window()

    try:
        headers, rows = fetch_all_pages(driver, URL, TARGET_PAGE)
        if not rows:
            raise ValueError("‚ùå Aucune donn√©e r√©cup√©r√©e.")
        write_csv(OUTPUT_CSV, headers, rows)
        print(f"‚úÖ {len(rows)} lignes enregistr√©es dans '{OUTPUT_CSV}'")
    except Exception:
        traceback.print_exc()
    finally:
        driver.quit()